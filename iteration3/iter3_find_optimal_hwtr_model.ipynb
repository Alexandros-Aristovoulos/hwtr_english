{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import everything\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import Sequential\n",
    "from keras.layers import Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "import time\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from numpy objects\n",
    "X_train = np.load('../my_dataset/X_train.npy')\n",
    "X_test = np.load('../my_dataset/X_test.npy')\n",
    "X_validate = np.load('../my_dataset/X_validate.npy')\n",
    "\n",
    "y_train = np.load('../my_dataset/y_train.npy')\n",
    "y_test = np.load('../my_dataset/y_test.npy')\n",
    "y_validate = np.load('../my_dataset/y_validate.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from the runs\n",
    "\n",
    "With the parameter restore_best_weights=True inside the EarlyStopping function the model keeps the weights of the best step. The training stops if after 6 stops we don't see an improvement.\n",
    "\n",
    "#### Equal Dropout\n",
    "* ##### 3 convolutional layers with 32 nodes each (Accuracy)\n",
    "<img src=\"iteration3_image_stats\\3_conv_32_convNodes_equalDropout_accuracy.svg\">\n",
    "\n",
    "| Colour    | Last Step | Model                                      | Validation accuracy   |\n",
    "| --------- | --------- | ------------------------------------------ | --------------------- |\n",
    "| Blue      | 27        | 3-conv-32-convNodes-0.05-equalDropout      | 88.75 %               |\n",
    "| Cyan      | 23        | 3-conv-32-convNodes-0.10-equalDropout      | 90.63 %               |\n",
    "| Green     | 24        | 3-conv-32-convNodes-0.15-equalDropout      | 91.25 %               |\n",
    "| Orange    | 21        | 3-conv-32-convNodes-0.20-equalDropout      | 88.13 %               |\n",
    "| Red       | 22        | 3-conv-32-convNodes-0.25-equalDropout      | 90.00 %               |\n",
    "| Pink      | 22        | 3-conv-32-convNodes-0.30-equalDropout      | 90.63 %               |\n",
    "\n",
    "* ##### 3 convolutional layers with 48 nodes each (Accuracy)\n",
    "<img src=\"iteration3_image_stats\\3_conv_48_convNodes_equalDropout_accuracy.svg\">\n",
    "\n",
    "| Colour    | Last Step | Model                                      | Validation accuracy   |\n",
    "| --------- | --------- | ------------------------------------------ | --------------------- |\n",
    "| White     | 22        | 3-conv-48-convNodes-0.05-equalDropout      | 90.00 %               |\n",
    "| **Blue**  | **24**    | **3-conv-48-convNodes-0.10-equalDropout**  | **93.12 %**           |\n",
    "| Cyan      | 16        | 3-conv-48-convNodes-0.15-equalDropout      | 90.63 %               |\n",
    "| **Green** | **26**    | **3-conv-48-convNodes-0.20-equalDropout**  | **95.00 %**           |\n",
    "| Orange    | 23        | 3-conv-48-convNodes-0.25-equalDropout      | 91.25 %               |\n",
    "| Red       | 23        | 3-conv-48-convNodes-0.30-equalDropout      | 91.87 %               |\n",
    "\n",
    "* ##### 4 convolutional layers with 32 nodes each (Accuracy)\n",
    "<img src=\"iteration3_image_stats\\4_conv_32_convNodes_equalDropout_accuracy.svg\">\n",
    "\n",
    "| Colour    | Last Step | Model                                      | Validation accuracy   |\n",
    "| --------- | --------- | ------------------------------------------ | --------------------- |\n",
    "| Pink      | 24        | 4-conv-32-convNodes-0.05-equalDropout      | 85.00 %               |\n",
    "| White     | 34        | 4-conv-32-convNodes-0.01-equalDropout      | 90.00 %               |\n",
    "| Blue      | 42        | 4-conv-32-convNodes-0.15-equalDropout      | 91.87 %               |\n",
    "| Cyan      | 29        | 4-conv-32-convNodes-0.20-equalDropout      | 90.00 %               |\n",
    "| Green     | 31        | 4-conv-32-convNodes-0.25-equalDropout      | 91.25 %               |\n",
    "| Orange    | 31        | 4-conv-32-convNodes-0.30-equalDropout      | 89.28 %               |\n",
    "\n",
    "* ##### 4 convolutional layers with 48 nodes each (Accuracy)\n",
    "<img src=\"iteration3_image_stats\\4_conv_48_convNodes_equalDropout_accuracy.svg\">\n",
    "\n",
    "| Colour    | Last Step | Model                                      | Validation accuracy   |\n",
    "| --------- | --------- | ------------------------------------------ | --------------------- |\n",
    "| Red       | 15        | 4-conv-48-convNodes-0.05-equalDropout      | 85.00 %               |\n",
    "| pink      | 24        | 4-conv-48-convNodes-0.10-equalDropout      | 90.00 %               |\n",
    "| **White** | **31**    | **4-conv-48-convNodes-0.15-equalDropout**  | **93.75 %**           |\n",
    "| Blue      | 26        | 4-conv-48-convNodes-0.20-equalDropout      | 90.00 %               |\n",
    "| Cyan      | 23        | 4-conv-48-convNodes-0.25-equalDropout      | 91.25 %               |\n",
    "| **Green** | **57**    | **4-conv-48-convNodes-0.30-equalDropout**  | **94.38 %**           |\n",
    "\n",
    "#### Linear Dropout\n",
    "* ##### 3 convolutional layers with 32 nodes each (Accuracy)\n",
    "<img src=\"iteration3_image_stats\\3_conv_32_convNodes_linearDropout_accuracy.svg\">\n",
    "\n",
    "| Colour    | Last Step | Model                                      | Validation accuracy   |\n",
    "| --------- | --------- | ------------------------------------------ | --------------------- |\n",
    "| Orange    | 14        | 3-conv-32-convNodes-0.10-linearDropout     | 86.87 %               |\n",
    "| Red       | 19        | 3-conv-32-convNodes-0.15-linearDropout     | 90.63 %               |\n",
    "| **Pink**  | **23**    | **3-conv-32-convNodes-0.20-linearDropout** | **93.12 %**           |\n",
    "| White     | 24        | 3-conv-32-convNodes-0.25-linearDropout     | 91.87 %               |\n",
    "| Blue      | 23        | 3-conv-32-convNodes-0.30-linearDropout     | 91.87 %               |\n",
    "\n",
    "* ##### 3 convolutional layers with 48 nodes each (Accuracy)\n",
    "<img src=\"iteration3_image_stats\\3_conv_48_convNodes_linearDropout_accuracy.svg\">\n",
    "\n",
    "| Colour    | Last Step | Model                                      | Validation accuracy   |\n",
    "| --------- | --------- | ------------------------------------------ | --------------------- |\n",
    "| **Cyan**  | **23**    | **3-conv-48-convNodes-0.10-linearDropout** | **92.50 %**           |\n",
    "| Green     | 22        | 3-conv-48-convNodes-0.15-linearDropout     | 88.75 %               |\n",
    "| Orange    | 17        | 3-conv-48-convNodes-0.20-linearDropout     | 91.87 %               |\n",
    "| **Red**   | **20**    | **3-conv-48-convNodes-0.25-linearDropout** | **93.12 %**           |\n",
    "| Pink      | 17        | 3-conv-48-convNodes-0.30-linearDropout     | 90.00 %               |\n",
    "\n",
    "* ##### 4 convolutional layers with 32 nodes each (Accuracy)\n",
    "<img src=\"iteration3_image_stats\\4_conv_32_convNodes_linearDropout_accuracy.svg\">\n",
    "\n",
    "| Colour    | Last Step | Model                                      | Validation accuracy   |\n",
    "| --------- | --------- | ------------------------------------------ | --------------------- |\n",
    "| White     | 22        | 4-conv-32-convNodes-0.10-linearDropout     | 85.00 %               |\n",
    "| Blue      | 35        | 4-conv-32-convNodes-0.15-linearDropout     | 90.00 %               |\n",
    "| Cyan      | 29        | 4-conv-32-convNodes-0.20-linearDropout     | 88.13 %               |\n",
    "| Green     | 29        | 4-conv-32-convNodes-0.25-linearDropout     | 89.38 %               |\n",
    "| Orange    | 26        | 4-conv-32-convNodes-0.30-linearDropout     | 88.13 %               |\n",
    "\n",
    "* ##### 4 convolutional layers with 48 nodes each (Accuracy)\n",
    "<img src=\"iteration3_image_stats\\4_conv_48_convNodes_linearDropout_accuracy.svg\">\n",
    "\n",
    "| Colour    | Last Step | Model                                      | Validation accuracy   |\n",
    "| --------- | --------- | ------------------------------------------ | --------------------- |\n",
    "| Red       | 27        | 4-conv-48-convNodes-0.10-linearDropout     | 87.50 %               |\n",
    "| Pink      | 7         | 4-conv-48-convNodes-0.15-linearDropout     | 06.25 %  (Error?!)    |\n",
    "| White     | 24        | 4-conv-48-convNodes-0.20-linearDropout     | 89.38 %               |\n",
    "| **Blue**  | **33**    | **4-conv-48-convNodes-0.25-linearDropout** | **94.38 %**           |\n",
    "| **Orange**| **37**    | **4-conv-48-convNodes-0.30-linearDropout** | **92.50 %**           |\n",
    "\n",
    "## Conclusions from the measurements:\n",
    "* 48 convolutional nodes perform better than 32\n",
    "* 0.05 equal dropout is too small to help\n",
    "* There need to be more runs to calculate the averages and determine a more clear winner (use model.evaluate function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [3, 4]\n",
    "conv_nodes = [32, 48]\n",
    "epochs = 200\n",
    "early_stops = 6\n",
    "\n",
    "dropouts = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "linear_dropouts = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "\n",
    "# equal dropout\n",
    "for conv_layer in conv_layers:\n",
    "    for conv_node in conv_nodes:\n",
    "        for dropout in dropouts:\n",
    "            NAME = \"{}-conv-{}-convNodes-{}-equalDropout-{}\".format(conv_layer, conv_node, dropout, int(time.time()))\n",
    "            print(NAME)\n",
    "\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(conv_node, (3, 3), input_shape=X_train.shape[1:]))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(Conv2D(conv_node, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "                model.add(Dropout(dropout))\n",
    "\n",
    "            model.add(Flatten())\n",
    "\n",
    "            # output layer (as big as the number of words we teach)\n",
    "            output_neurons = len(np.unique(y_train))\n",
    "            model.add(tf.keras.layers.Dense(output_neurons, activation=tf.nn.softmax))\n",
    "\n",
    "            tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "            model.compile(loss='sparse_categorical_crossentropy',\n",
    "                        optimizer='adam',\n",
    "                        metrics=['accuracy'],\n",
    "                        )\n",
    "\n",
    "            callbacks = [EarlyStopping(monitor='val_accuracy', patience=early_stops, restore_best_weights=True), tensorboard]\n",
    "            model.fit(X_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_validate, y_validate),\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "            # save the model\n",
    "            model.save(\"models/\"+NAME)\n",
    "\n",
    "\n",
    "# linear dropout\n",
    "for conv_layer in conv_layers:\n",
    "    for conv_node in conv_nodes:\n",
    "        for dropout in linear_dropouts:\n",
    "            NAME = \"{}-conv-{}-convNodes-{}-linearDropout-{}\".format(conv_layer, conv_node, dropout, int(time.time()))\n",
    "            print(NAME)\n",
    "\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(conv_node, (3, 3), input_shape=X_train.shape[1:]))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(Conv2D(conv_node, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "                model.add(Dropout(dropout/(l+1)))\n",
    "\n",
    "            model.add(Flatten())\n",
    "\n",
    "            # output layer (as big as the number of words we teach)\n",
    "            output_neurons = len(np.unique(y_train))\n",
    "            model.add(tf.keras.layers.Dense(output_neurons, activation=tf.nn.softmax))\n",
    "\n",
    "            tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "            model.compile(loss='sparse_categorical_crossentropy',\n",
    "                        optimizer='adam',\n",
    "                        metrics=['accuracy'],\n",
    "                        )\n",
    "\n",
    "            callbacks = [EarlyStopping(monitor='val_accuracy', patience=early_stops, restore_best_weights=True), tensorboard]\n",
    "            model.fit(X_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_validate, y_validate),\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "            # save the model\n",
    "            model.save(\"models/\"+NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99aa377fbfe6babaa8f85c06716341a20688fb45a5f56fcc96594486c2d6abfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
