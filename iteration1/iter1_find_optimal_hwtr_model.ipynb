{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import everything\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import TensorBoard\n",
    "import time\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from numpy objects\n",
    "X_train = np.load('../my_dataset/X_train.npy')\n",
    "X_test = np.load('../my_dataset/X_test.npy')\n",
    "X_validate = np.load('../my_dataset/X_validate.npy')\n",
    "\n",
    "y_train = np.load('../my_dataset/y_train.npy')\n",
    "y_test = np.load('../my_dataset/y_test.npy')\n",
    "y_validate = np.load('../my_dataset/y_validate.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions from the runs (Iteration 1)\n",
    "\n",
    "### 1. The accuracy from the train data cannot be used to judge the efficiency of a model\n",
    "Since the model uses the train data to train, it can overfit to these specific data and end up with an accuracy of a 100%.\n",
    "However, when we provide the same model with different data (validation data) and ask it to make predictions we will notice\n",
    "a significantly lower accuracy rate since it hasn't \"memorised\" these new data. For this reason only the accuracy from the\n",
    "validation data will be taken into consideration when choosing the better model.\n",
    "\n",
    "### 2. Best peforming models\n",
    "From the validation accuracy graph we filter and keep the 8 best performing runs out of the 36 (best 25%). And we get the following results: <br> <br>\n",
    "<img src=\"iteration1_image_stats\\iteration1_accuracy.svg\">\n",
    "| Model                     | Validation accuracy   |\n",
    "| -----------               | -----------           |\n",
    "| 3-conv-32-nodes-0-dense   | 88.75 %               |\n",
    "| 3-conv-16-nodes-1-dense   | 86.25 %               |\n",
    "| 3-conv-32-nodes-1-dense   | 85.62 %               |\n",
    "| 3-conv-32-nodes-2-dense   | 85.00 %               |\n",
    "| 3-conv-16-nodes-0-dense   | 83.75 %               |\n",
    "| 2-conv-16-nodes-0-dense   | 82.50 %               |\n",
    "| 3-conv-16-nodes-2-dense   | 81.25 %               |\n",
    "| 1-conv-16-nodes-0-dense   | 81.25 %               |\n",
    "\n",
    "* The majority of the best performing models had 3 convolutional layers.<br> \n",
    "Does the accuracy increase further if we **increase the number of the convolutional layers**?<br> \n",
    "\n",
    "* Models with more nodes generally performed better.<br> \n",
    "Does the accuracy increase further if we **increase the number of nodes per layer**? \n",
    "\n",
    "* Models with less dense layers generally performed better. These layers serve as memory and it seems that with less of them the model would memorise less and generalise more. However, it seems that a little bit of memory does help. Models with less nodes (16) and 1 dense layer performed better than the same model without a dense layer (3-conv-16-nodes-1-dense (86.25 %) VS  3-conv-16-nodes-0-dense (83.75 %)). During testing the convolutional layers and the dense layers all had the same amount of nodes.<br>\n",
    "Does the accuracy increase further if we **use different nodes per type of layer** and if we use **less nodes for the dense layers than the convolutional layers**?<br> \n",
    "\n",
    "### 3. Validation loss graph\n",
    "From the graph we can see that the loss decreases until the 25-30th epoch and then it starts to slowly increase again. For this reason we will decrease the number of epochs from 50 to 45 and we may adjust further based on the new results.<br> <br> \n",
    "<img src=\"iteration1_image_stats\\iteration1_loss.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [0, 1, 2]\n",
    "layer_sizes = [4, 8, 16, 32]\n",
    "conv_layers = [1, 2, 3]\n",
    "epochs = 50\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "                NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "                print(NAME)\n",
    "\n",
    "                model = Sequential()\n",
    "\n",
    "                model.add(Conv2D(layer_size, (3, 3), input_shape=X_train.shape[1:]))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "                for l in range(conv_layer-1):\n",
    "                    model.add(Conv2D(layer_size, (3, 3)))\n",
    "                    model.add(Activation('relu'))\n",
    "                    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "                model.add(Flatten())\n",
    "\n",
    "                for _ in range(dense_layer):\n",
    "                    model.add(Dense(layer_size))\n",
    "                    model.add(Activation('relu'))\n",
    "\n",
    "                # output layer (as big as the number of words we teach)\n",
    "                output_neurons = len(np.unique(y_train))\n",
    "                model.add(tf.keras.layers.Dense(output_neurons, activation=tf.nn.softmax))\n",
    "\n",
    "                tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "                model.compile(loss='sparse_categorical_crossentropy',\n",
    "                            optimizer='adam',\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "                model.fit(X_train, y_train,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(X_validate, y_validate),\n",
    "                        callbacks=[tensorboard])\n",
    "\n",
    "                # save the model\n",
    "                model.save(\"models/\"+NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99aa377fbfe6babaa8f85c06716341a20688fb45a5f56fcc96594486c2d6abfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
